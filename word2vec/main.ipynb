{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444dc47e",
   "metadata": {},
   "source": [
    "## Лабораторная работа №1\n",
    "\n",
    "**Тема:** word2vec и TF-IDF\n",
    "\n",
    "**Выполнил:** Студент группы БВТ2201 Шамсутдинов Рустам Фаргатевич\n",
    "\n",
    "**Цель лабораторной работы:** Изучить и реализовать алгоритмы word2vec и TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24dbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39611655",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a352a",
   "metadata": {},
   "source": [
    "Загрузка и подготовка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sport', 'car']\n",
      "['fair', 'number', 'brave', 'soul', 'upgraded', 'clock', 'oscillator', 'shared', 'experience', 'poll']\n",
      "['well', 'folk', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life']\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "LEMMA = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # 1) Удаляем HTML-теги\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # 2) Удаляем URL\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    # 3) Ловим только английские буквы (удаляем цифры, пунктуацию)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    # 4) Сводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # 5) Нормализуем пробелы\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        # убираем короткие токены и стоп-слова\n",
    "        if len(tok) < 3 or tok in STOPWORDS:\n",
    "            continue\n",
    "        # лемматизируем\n",
    "        lemma = LEMMA.lemmatize(tok)\n",
    "        result.append(lemma)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Применение к DataFrame\n",
    "newsgroups = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "docs = newsgroups.data[:200]\n",
    "\n",
    "sentences = [tokenize(d) for d in docs]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"text\": docs})\n",
    "df[\"tokens\"] = df[\"text\"].apply(tokenize)\n",
    "\n",
    "# Посмотрим примеры\n",
    "for i in range(3):\n",
    "    print(df.loc[i, \"tokens\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577b375",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens):\n",
    "    counts = Counter(tokens)\n",
    "    total = sum(counts.values())\n",
    "    return {w: c / total for w, c in counts.items()}\n",
    "\n",
    "\n",
    "df[\"tf\"] = df[\"tokens\"].apply(compute_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8bd6a",
   "metadata": {},
   "source": [
    "Посчитать IDF по всему корпусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec519ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "df_unique = df[\"tokens\"].apply(set)\n",
    "df_counts = defaultdict(int)\n",
    "for uniq in df_unique:\n",
    "    for w in uniq:\n",
    "        df_counts[w] += 1\n",
    "\n",
    "idf = {w: math.log(N / df_counts[w]) for w in df_counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5f59e",
   "metadata": {},
   "source": [
    "Собрать TF-IDF вектора (как словарь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6e4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: топ-5 терминов первого документа:\n",
      "Слово: car, его значение: 0.2698067063953178\n",
      "Слово: door, его значение: 0.24643336588595519\n",
      "Слово: enlighten, его значение: 0.12321668294297759\n",
      "Слово: bricklin, его значение: 0.12321668294297759\n",
      "Слово: tellme, его значение: 0.12321668294297759\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(tf_dict):\n",
    "    return {w: tf_dict[w] * idf[w] for w in tf_dict}\n",
    "\n",
    "\n",
    "df[\"tfidf\"] = df[\"tf\"].apply(compute_tfidf)\n",
    "\n",
    "# Пример: распечатаем топ-5 слов по tf-idf в первом документе\n",
    "first_tfidf = df.loc[0, \"tfidf\"]\n",
    "top5 = sorted(first_tfidf.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"TF-IDF: топ-5 терминов первого документа:\")\n",
    "for term, value in top5:\n",
    "    print(f\"Слово: {term}, его значение: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda59046",
   "metadata": {},
   "source": [
    "Простая Word2Vec (skip-gram без негативного сэмплинга)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8386b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastWord2Vec:\n",
    "    def __init__(self, sentences, window=2, dim=50, lr=0.025, epochs=3, neg_samples=5):\n",
    "        # Строим словарь\n",
    "        vocab = sorted({w for sent in sentences for w in sent})\n",
    "        self.w2i = {w: i for i, w in enumerate(vocab)}\n",
    "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
    "        self.V = len(vocab)\n",
    "        self.dim = dim\n",
    "        self.window = window\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.neg = neg_samples\n",
    "\n",
    "        # Инициализация весов\n",
    "        self.W_in = np.random.randn(self.V, dim) * 0.01\n",
    "        self.W_out = np.zeros((dim, self.V))\n",
    "\n",
    "        # Распределение для negative sampling\n",
    "        freq = Counter([w for sent in sentences for w in sent])\n",
    "        pow_freq = np.array([freq[self.i2w[i]] ** 0.75 for i in range(self.V)])\n",
    "        self.neg_dist = pow_freq / pow_freq.sum()\n",
    "\n",
    "        # Корпус в индексах\n",
    "        self.data = [[self.w2i[w] for w in sent] for sent in sentences]\n",
    "\n",
    "    def train(self):\n",
    "        for ep in range(1, self.epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for sent in self.data:\n",
    "                for idx, target in enumerate(sent):\n",
    "                    # контекстное окно\n",
    "                    start = max(idx - self.window, 0)\n",
    "                    end = min(idx + self.window + 1, len(sent))\n",
    "                    contexts = [sent[i] for i in range(start, end) if i != idx]\n",
    "\n",
    "                    v_in = self.W_in[target]\n",
    "                    for ctx in contexts:\n",
    "                        # негативные образцы\n",
    "                        negs = list(\n",
    "                            np.random.choice(self.V, size=self.neg, p=self.neg_dist)\n",
    "                        )\n",
    "                        samples = [ctx] + negs\n",
    "                        labels = np.array([1] + [0] * self.neg)\n",
    "\n",
    "                        vecs = self.W_out[:, samples]  # (dim, neg+1)\n",
    "                        dots = v_in.dot(vecs)  # (neg+1,)\n",
    "                        probs = 1 / (1 + np.exp(-dots))  # sigmoid\n",
    "\n",
    "                        error = probs - labels\n",
    "                        total_loss += -np.sum(\n",
    "                            labels * np.log(probs + 1e-8)\n",
    "                            + (1 - labels) * np.log(1 - probs + 1e-8)\n",
    "                        )\n",
    "\n",
    "                        # обновляем W_out и W_in\n",
    "                        self.W_out[:, samples] -= self.lr * np.outer(v_in, error)\n",
    "                        self.W_in[target] -= self.lr * (vecs.dot(error))\n",
    "            print(f\"Epoch {ep}/{self.epochs}, loss={total_loss:.4f}\")\n",
    "\n",
    "    def vector(self, word):\n",
    "        idx = self.w2i.get(word)\n",
    "        if idx is None:\n",
    "            raise ValueError(f\"'{word}' отсутствует в словаре\")\n",
    "        return self.W_in[idx]\n",
    "\n",
    "    def most_similar(self, word, topn=5):\n",
    "        if word not in self.w2i:\n",
    "            raise ValueError(f\"Слово '{word}' отсутствует в словаре.\")\n",
    "        v = self.vector(word)\n",
    "        sims = np.dot(self.W_in, v) / (\n",
    "            np.linalg.norm(self.W_in, axis=1) * np.linalg.norm(v) + 1e-9\n",
    "        )\n",
    "        best = np.argsort(-sims)\n",
    "        result = []\n",
    "        for idx in best:\n",
    "            w = self.i2w[idx]\n",
    "            if w != word:\n",
    "                result.append((w, sims[idx]))\n",
    "            if len(result) >= topn:\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def most_similar_vector(self, vector, topn=5):\n",
    "        sims = np.dot(self.W_in, vector) / (\n",
    "            np.linalg.norm(self.W_in, axis=1) * np.linalg.norm(vector) + 1e-9\n",
    "        )\n",
    "        best = np.argsort(-sims)\n",
    "        result = []\n",
    "        for idx in best:\n",
    "            w = self.i2w[idx]\n",
    "            result.append((w, sims[idx]))\n",
    "            if len(result) >= topn:\n",
    "                break\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4c4e6",
   "metadata": {},
   "source": [
    "Демонстрация работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss=291331.6168\n",
      "Epoch 2/5, loss=267609.1142\n",
      "Epoch 3/5, loss=220984.4192\n",
      "Epoch 4/5, loss=196768.6364\n",
      "Epoch 5/5, loss=185811.3838\n"
     ]
    }
   ],
   "source": [
    "model = FastWord2Vec(sentences, window=2, dim=50, lr=0.05, epochs=5, neg_samples=5)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Слова: 'king' vs 'woman'\n",
      "Разность (первые 5): [-0.19782094  0.05931985  0.10631466  0.12400341 -0.11832959]\n",
      "Сумма   (первые 5): [ 0.61607477  0.23506817  0.47135306 -0.84110859  0.83965184]\n"
     ]
    }
   ],
   "source": [
    "# w1, w2 = random.sample(list(model.w2i.keys()), 2)\n",
    "w1 = \"king\"\n",
    "w2 = \"woman\"\n",
    "v1, v2 = model.vector(w1), model.vector(w2)\n",
    "\n",
    "diff = v1 - v2\n",
    "summ = v1 + v2\n",
    "\n",
    "print(f\"\\nСлова: '{w1}' vs '{w2}'\")\n",
    "print(\"Разность (первые 5):\", diff[:5])\n",
    "print(\"Сумма   (первые 5):\", summ[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7032c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-5 слов, близких к вектору разности 'king' - 'woman':\n",
      "rlk: 0.4465\n",
      "ltq: 0.4135\n",
      "pmf: 0.3699\n",
      "whjn: 0.3611\n",
      "fyn: 0.3600\n"
     ]
    }
   ],
   "source": [
    "similar_words_diff = model.most_similar_vector(diff, topn=5)\n",
    "print(f\"Топ-5 слов, близких к вектору разности '{w1}' - '{w2}':\")\n",
    "for word, score in similar_words_diff:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2830da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-5 слов, близких к вектору суммы 'king' + 'woman':\n",
      "swear: 0.9941\n",
      "turned: 0.9929\n",
      "story: 0.9914\n",
      "home: 0.9910\n",
      "whether: 0.9910\n"
     ]
    }
   ],
   "source": [
    "similar_words_diff = model.most_similar_vector(summ, topn=5)\n",
    "print(f\"Топ-5 слов, близких к вектору суммы '{w1}' + '{w2}':\")\n",
    "for word, score in similar_words_diff:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d676de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тeстирование нескольких операций\n",
      "woman: 0.9876\n",
      "never: 0.9665\n",
      "child: 0.9654\n",
      "kurdish: 0.9631\n",
      "hundred: 0.9629\n"
     ]
    }
   ],
   "source": [
    "w1 = \"king\"\n",
    "w2 = \"woman\"\n",
    "w3 = \"man\"\n",
    "v1, v2, v3 = model.vector(w1), model.vector(w2), model.vector(w3)\n",
    "test = v1 - v3 + v2\n",
    "\n",
    "similar_words_test = model.most_similar_vector(test, topn=5)\n",
    "print(f\"Тeстирование нескольких операций\")\n",
    "for word, score in similar_words_test:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
