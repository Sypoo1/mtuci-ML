{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №3\n",
    "\n",
    "**Тема:** RNN и Text classification\n",
    "\n",
    "**Выполнил:** Студент группы БВТ2201 Шамсутдинов Рустам Фаргатевич\n",
    "\n",
    "**Цель лабораторной работы:** Создать модель с архитектурой RNN для классификации текста на токсичный и не токсичный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14412, 2)\n",
      "toxic\n",
      "0.0    9586\n",
      "1.0    4826\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"russian_toxic_comments.csv\")\n",
    "print(df.shape)\n",
    "print(df[\"toxic\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples — train: 10376, val: 1153, test: 2883\n"
     ]
    }
   ],
   "source": [
    "train_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"toxic\"])\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.1, stratify=train_val_df[\"toxic\"]\n",
    ")\n",
    "\n",
    "print(f\"Samples — train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(0),  # [max_len]\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(0),  # [max_len]\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # 0.0 / 1.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ToxicDataset(train_df[\"comment\"], train_df[\"toxic\"], tokenizer),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ToxicDataset(val_df[\"comment\"], val_df[\"toxic\"], tokenizer),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ToxicDataset(test_df[\"comment\"], test_df[\"toxic\"], tokenizer),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim=256,\n",
    "        rnn_hidden=128,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        dropout_emb=0.3,\n",
    "        dropout_rnn=0.3,\n",
    "        dropout_fc=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.drop_emb = nn.Dropout(dropout_emb)\n",
    "\n",
    "        # LSTM с drop между слоями (только если num_layers > 1)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rnn if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self.drop_fc = nn.Dropout(dropout_fc)\n",
    "        self.fc = nn.Linear(rnn_hidden * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "        emb = self.drop_emb(emb)\n",
    "\n",
    "        out, (h_n, _) = self.rnn(emb)\n",
    "\n",
    "        # собираем финальное скрытое состояние\n",
    "        if self.rnn.bidirectional:\n",
    "            h_forward = h_n[-2]\n",
    "            h_backward = h_n[-1]\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        h_final = self.drop_fc(h_final)\n",
    "        logits = self.fc(h_final).squeeze(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ToxicRNN(vocab_size=tokenizer.vocab_size).to(device)\n",
    "\n",
    "# балансировка классов\n",
    "pos = train_df[\"toxic\"].sum()\n",
    "neg = len(train_df) - pos\n",
    "pos_weight = torch.tensor([neg / pos], device=device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            lbls = batch[\"label\"].cpu().numpy()\n",
    "            logits = model(ids)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            ys.extend(lbls)\n",
    "            ps.extend(probs)\n",
    "    ys = np.array(ys)\n",
    "    ps = np.array(ps)\n",
    "    acc = accuracy_score(ys, ps > 0.5)\n",
    "    f1 = f1_score(ys, ps > 0.5)\n",
    "    roc_auc = roc_auc_score(ys, ps)\n",
    "    prec, rec, _ = precision_recall_curve(ys, ps)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    return acc, f1, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 325/325 [01:31<00:00,  3.55it/s, train_loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 | val acc: 0.7051 | F1: 0.5923 | ROC-AUC: 0.7420 | PR-AUC: 0.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 325/325 [01:30<00:00,  3.60it/s, train_loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2 | val acc: 0.7823 | F1: 0.6203 | ROC-AUC: 0.8309 | PR-AUC: 0.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 325/325 [01:30<00:00,  3.60it/s, train_loss=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3 | val acc: 0.8083 | F1: 0.6736 | ROC-AUC: 0.8702 | PR-AUC: 0.7977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 325/325 [01:29<00:00,  3.61it/s, train_loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4 | val acc: 0.8205 | F1: 0.7251 | ROC-AUC: 0.8754 | PR-AUC: 0.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 325/325 [01:29<00:00,  3.63it/s, train_loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5 | val acc: 0.8413 | F1: 0.7483 | ROC-AUC: 0.8931 | PR-AUC: 0.8450\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in loop:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        lbls = batch[\"label\"].to(device)\n",
    "        logits = model(ids)\n",
    "        loss = criterion(logits, lbls)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        loop.set_postfix(train_loss=np.mean(epoch_losses))\n",
    "\n",
    "    val_acc, val_f1, val_roc, val_pr = evaluate(val_loader)\n",
    "    print(\n",
    "        f\"→ Epoch {epoch} | val acc: {val_acc:.4f} | F1: {val_f1:.4f} \"\n",
    "        f\"| ROC-AUC: {val_roc:.4f} | PR-AUC: {val_pr:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST METRICS ===\n",
      "Acc: 0.8526 | F1: 0.7653 | ROC-AUC: 0.9051 | PR-AUC: 0.8474\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_f1, test_roc, test_pr = evaluate(test_loader)\n",
    "print(\"\\n=== FINAL TEST METRICS ===\")\n",
    "print(\n",
    "    f\"Acc: {test_acc:.4f} | F1: {test_f1:.4f} | \"\n",
    "    f\"ROC-AUC: {test_roc:.4f} | PR-AUC: {test_pr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,   947, 19142,   177,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Text: «Ты дурак!»  →  Toxic? 1  (p=0.972)\n",
      "tensor([[ 101, 7953,  681, 3877,  126,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Text: «Спасибо за помощь.»  →  Toxic? 0  (p=0.067)\n",
      "tensor([[   101,    789,    789,    789,    947,   1179, 107349,    378,    177,\n",
      "            102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "Text: «да да да ты очень умный!»  →  Toxic? 1  (p=0.990)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128  # то же, что при обучении\n",
    "\n",
    "\n",
    "def predict_one(text: str):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    print(input_ids)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        label = 1 if prob > 0.5 else 0\n",
    "\n",
    "    return label, prob\n",
    "\n",
    "\n",
    "text1 = \"Ты дурак!\"\n",
    "label, confidence = predict_one(text1)\n",
    "print(f\"Text: «{text1}»  →  Toxic? {label}  (p={confidence:.3f})\")\n",
    "\n",
    "text2 = \"Спасибо за помощь.\"\n",
    "label, confidence = predict_one(text2)\n",
    "print(f\"Text: «{text2}»  →  Toxic? {label}  (p={confidence:.3f})\")\n",
    "\n",
    "\n",
    "text3 = \"да да да ты очень умный!\"\n",
    "label, confidence = predict_one(text3)\n",
    "print(f\"Text: «{text3}»  →  Toxic? {label}  (p={confidence:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
